{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append root directory to system's path\n",
    "import sys\n",
    "sys.path.append('../ARCH_package')\n",
    "\n",
    "import plot, modelling, basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import os\n",
    "import json \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import isclose\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "colors = px.colors.qualitative.D3  # create a list of plotly colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load path to Other plots\n",
    "path = '../Results/Other/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import non-synonymoous trajectories as exported through with basic.load module\n",
    "with open('../Exports/LBC_non-synonymous.dill', 'rb') as infile:\n",
    "    lbc = dill.load(infile)\n",
    "\n",
    "# Import non-synonymoous trajectories as exported through with basic.load module\n",
    "with open('../Exports/LBC_synonymous.dill', 'rb') as infile:\n",
    "    syn = dill.load(infile)\n",
    "    \n",
    "# load NGF fitted trajectories \n",
    "with open('../Exports/neutral_filtered_trajectories.dill','rb') as infile:\n",
    "    model = dill.load(infile)\n",
    "\n",
    "# load Threshold fitted trajectories\n",
    "with open('../Exports/threshold_filtered_trajectories.dill','rb') as infile:\n",
    "    model_threshold = dill.load(infile)\n",
    "\n",
    "# load NGF cohort\n",
    "with open('../Exports/cohort_neutral.dill','rb') as infile:\n",
    "    cohort = dill.load(infile)\n",
    "    \n",
    "# load NGF fit information\n",
    "with open('../Exports/neutral_fit.dill','rb') as infile:\n",
    "    neutral_fit = dill.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2% datset overview\n",
    "## Gene trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nThe kaleido executable is required by the kaleido Python library, but it was not included\nin the Python package and it could not be found on the system PATH.\n\nSearched for included kaleido executable at:\n    /home/elatorre/anaconda3/envs/ARCH_env/lib/python3.9/site-packages/kaleido/executable/kaleido \n\nSearched for executable 'kaleido' on the following system PATH:\n    /home/elatorre/.local/bin\n    /usr/local/sbin\n    /usr/local/bin\n    /usr/sbin\n    /usr/bin\n    /sbin\n    /bin\n    /usr/games\n    /usr/local/games\n    /snap/bin\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_146053/1195088748.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgene_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgene\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#fig.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'Neil {gene}.svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ARCH_env/lib/python3.9/site-packages/plotly/basedatatypes.py\u001b[0m in \u001b[0;36mwrite_image\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3819\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3821\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3823\u001b[0m     \u001b[0;31m# Static helpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ARCH_env/lib/python3.9/site-packages/plotly/io/_kaleido.py\u001b[0m in \u001b[0;36mwrite_image\u001b[0;34m(fig, file, format, scale, width, height, validate, engine)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# -------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;31m# Do this first so we don't create a file if image conversion fails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m     img_data = to_image(\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ARCH_env/lib/python3.9/site-packages/plotly/io/_kaleido.py\u001b[0m in \u001b[0;36mto_image\u001b[0;34m(fig, format, width, height, scale, validate, engine)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# ---------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_coerce_fig_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     img_bytes = scope.transform(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/envs/ARCH_env/lib/python3.9/site-packages/kaleido/scopes/plotly.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, figure, format, width, height, scale)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# Transform in using _perform_transform rather than superclass so we can access the full\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# response dict, including error codes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response = self._perform_transform(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/ARCH_env/lib/python3.9/site-packages/kaleido/scopes/base.py\u001b[0m in \u001b[0;36m_perform_transform\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \"\"\"\n\u001b[1;32m    292\u001b[0m         \u001b[0;31m# Ensure that kaleido subprocess is running\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_kaleido\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# Perform export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ARCH_env/lib/python3.9/site-packages/kaleido/scopes/base.py\u001b[0m in \u001b[0;36m_ensure_kaleido\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m                     \u001b[0;31m# spaces.  The subprocess.Popen docs makes it sound like this shouldn't be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0;31m# necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                     \u001b[0mproc_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_proc_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                     self._proc = subprocess.Popen(\n\u001b[1;32m    178\u001b[0m                         \u001b[0mproc_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ARCH_env/lib/python3.9/site-packages/kaleido/scopes/base.py\u001b[0m in \u001b[0;36m_build_proc_args\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \"\"\"\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mproc_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scope_flags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ARCH_env/lib/python3.9/site-packages/kaleido/scopes/base.py\u001b[0m in \u001b[0;36mexecutable_path\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PATH\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mformatted_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpathsep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n    \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    100\u001b[0m                     \"\"\"\n\u001b[1;32m    101\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mkaleido\u001b[0m \u001b[0mexecutable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mrequired\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mkaleido\u001b[0m \u001b[0mPython\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwas\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mincluded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: \nThe kaleido executable is required by the kaleido Python library, but it was not included\nin the Python package and it could not be found on the system PATH.\n\nSearched for included kaleido executable at:\n    /home/elatorre/anaconda3/envs/ARCH_env/lib/python3.9/site-packages/kaleido/executable/kaleido \n\nSearched for executable 'kaleido' on the following system PATH:\n    /home/elatorre/.local/bin\n    /usr/local/sbin\n    /usr/local/bin\n    /usr/sbin\n    /usr/bin\n    /sbin\n    /bin\n    /usr/games\n    /usr/local/games\n    /snap/bin\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'../Datasets/LBC_ARCHER.2PCT_VAF.Mar21.non-synonymous.tsv', sep='\\t')\n",
    "\n",
    "for gene in ['TET2', 'DNMT3A', 'JAK2']:\n",
    "    fig = plot.gene_plot(df, gene)\n",
    "    fig.show()\n",
    "    fig.write_image(path + f'Neil {gene}.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene and variant counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load variant color dictionary from resources\n",
    "with open('../Resources/var_dict.json') as json_file:\n",
    "    var_dict = json.load(json_file)\n",
    "\n",
    "gene_dict = {element: 0 \n",
    "             for element in set(df.PreferredSymbol.unique())}\n",
    "var_count_dict = {element: 0 \n",
    "                for element in set(df.Variant_Classification.unique())}\n",
    "# create list of all keys\n",
    "\n",
    "for part in df.participant_id.unique():\n",
    "    data = df[df['participant_id']==part] \n",
    "    for key in data.key.unique():\n",
    "        data_key = data[data['key']==key]\n",
    "        gene = data_key.PreferredSymbol.unique()[0]\n",
    "        variant = data_key.Variant_Classification.unique()[0]\n",
    "        # update dict\n",
    "        gene_dict[gene] += 1 \n",
    "        var_count_dict[variant] += 1\n",
    "        \n",
    "# sort dictionary in descending order\n",
    "gene_dict = dict(sorted(gene_dict.items(),\n",
    "                        key=lambda item: item[1], reverse=True))\n",
    "var_count_dict = dict(sorted(var_count_dict.items(),\n",
    "                        key=lambda item: item[1]))\n",
    "\n",
    "# Bar plot\n",
    "fig = go.Figure()\n",
    "for key, item in gene_dict.items():\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=[key], y=[item],\n",
    "               name=f'{key}',\n",
    "               marker_color='Grey',\n",
    "               showlegend=False))\n",
    "fig.update_layout(\n",
    "            template=\"simple_white\",\n",
    "            yaxis_title='Count',\n",
    "            xaxis_tickangle=-45)\n",
    "fig.show()\n",
    "fig.write_image(path + 'gene_counts_2%.svg', width=800, height=400)\n",
    "\n",
    "var_name_dict = dict()\n",
    "for key in var_count_dict.keys():\n",
    "    new_key = key.replace('_', ' ')\n",
    "    new_key = new_key.replace('Ins', 'Insertion')\n",
    "    \n",
    "    # create dictionary of names\n",
    "    var_name_dict[key] = new_key\n",
    "    \n",
    "# Bar plot\n",
    "fig = go.Figure()\n",
    "for key, item in var_count_dict.items():\n",
    "    fig.add_trace(\n",
    "        go.Bar(y=[var_name_dict[key]], x=[item],\n",
    "               marker_color=var_dict[key],\n",
    "               showlegend=False,\n",
    "               orientation='h'))\n",
    "fig.update_layout(\n",
    "            template=\"simple_white\",\n",
    "            xaxis_title='Count',\n",
    "            )\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(path + 'variant_counts_2%.svg', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datapoints per trajectory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of DIF selected trajectories with more than 2 data points \n",
    "datapoints = [len(traj.x) for traj in cohort.model] \n",
    "\n",
    "percentage = len([i for i in datapoints if i>2])*100/len(datapoints)\n",
    "\n",
    "print(f'{round(percentage)}% of all fit trajectories have 3 or more timepoints.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class participant:\n",
    "    def __init__ (self, id=None, max_fitness=0, max_VAF=0, max_aux=0):\n",
    "        self.id = id\n",
    "        self.max_fitness = max_fitness\n",
    "        self.max_VAF = max_VAF\n",
    "        self.max_aux = max_aux\n",
    "\n",
    "part_list= []\n",
    "for traj in model:\n",
    "    part_list.append(traj.id)\n",
    "part_list = list(set(part_list))\n",
    "\n",
    "part_class = []\n",
    "for part in part_list:\n",
    "    part_class.append(participant(id=part))\n",
    "\n",
    "# Extract maximum VAF, fitness, auxiliary for each participant\n",
    "for traj in model:\n",
    "    part = [x for x in part_class if x.id == traj.id][0]\n",
    "    \n",
    "    # update participant values\n",
    "    mean_vaf = np.mean(traj.y)\n",
    "    part.max_VAF = max(mean_vaf, part.max_VAF)\n",
    "    part.max_fitness = max(traj.fitness, part.max_fitness)\n",
    "    part.max_aux = max (traj.fitness*mean_vaf, part.max_aux)\n",
    "\n",
    "vaf = [part.max_VAF for part in part_class]\n",
    "fig = go.Figure(\n",
    "        go.Box(y=vaf, boxmean= True, boxpoints='all'))\n",
    "fig.update_layout(template = 'simple_white',\n",
    "                  title='Highest mean VAF')\n",
    "fig.update_yaxes(title='VAF')\n",
    "fig.show()\n",
    "fig.write_image(path + 'highest_VAF.svg', width=500)\n",
    "\n",
    "fitness = [part.max_fitness for part in part_class]\n",
    "print(f'Median maximum fitness per participant: {round(np.median(fitness),3)}')\n",
    "\n",
    "fig = go.Figure(\n",
    "        go.Box(y=fitness, boxmean= True, boxpoints='all'))\n",
    "fig.update_layout(template = 'simple_white',\n",
    "                  title='Highest Fitness')\n",
    "fig.update_yaxes(title='Fitness')\n",
    "fig.show()\n",
    "fig.write_image(path + 'highest_fitness.svg', width=500)\n",
    "\n",
    "\n",
    "aux = [part.max_aux for part in part_class]\n",
    "print(f'Median maximum fitness*VAF per participant: {round(np.median(aux),3)}')\n",
    "\n",
    "fig = go.Figure(\n",
    "        go.Box(y=aux, boxmean= True, boxpoints='all'))\n",
    "fig.update_layout(template = 'simple_white',\n",
    "                  title='Highest fitnees x VAF')\n",
    "fig.update_yaxes(title='fitness x VAF')\n",
    "fig.show()\n",
    "fig.write_image(path +'highest_fitness_VAF.svg', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average time step in synonymous mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "melt_syn = pd.DataFrame(columns=['AF', 'regularized_gradient', 'timestep'])\n",
    "\n",
    "for part in syn:\n",
    "    for traj in part.trajectories:\n",
    "        for combination in list(combinations(traj.data.index, 2)):\n",
    "            data = traj.data.loc[list(combination)]\n",
    "            gradient = np.diff(data.AF) / np.sqrt(np.diff(data.age))\n",
    "            melt_syn = (\n",
    "                melt_syn.append({'AF': traj.data.loc[combination[0]]['AF'],\n",
    "                                 'regularized_gradient': gradient[0],\n",
    "                                 'timestep':np.sqrt(np.diff(data.age))[0]},\n",
    "                                ignore_index=True))\n",
    "\n",
    "# Exclude all mutations with VAF < 0.01\n",
    "# lack of data below this threshold alters the distribution of gradients\n",
    "filtered_melt_syn = melt_syn[melt_syn['AF'] > 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_step = np.mean(filtered_melt_syn['timestep'])\n",
    "(f'Average time-step in synonymous mutation trajectories used '\n",
    " f'for DIF filter training: {round(mean_step,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSC estimate\n",
    "## 1. Size of synonymous variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaf_list = []\n",
    "age_list = []\n",
    "for part in syn:\n",
    "    max_vaf = 0\n",
    "    age = []\n",
    "    for traj in part.trajectories:\n",
    "        if np.mean(traj.data.AF) > max_vaf:\n",
    "            # Update max_vaf\n",
    "            max_vaf = np.mean(traj.data.AF)\n",
    "            # append age for each trajectory\n",
    "            max_age = np.mean(traj.data['age'])\n",
    "    if max_vaf != 0:\n",
    "        # Update vaf and age lists \n",
    "        vaf_list.append(max_vaf)\n",
    "        age_list.append(max_age)\n",
    "\n",
    "print(f'Mean max_vaf: {np.mean(vaf_list)}')\n",
    "print(f'Mean age: {np.mean(age_list)}')\n",
    "fig = go.Figure(\n",
    "        go.Box(y=vaf_list, boxmean= True, boxpoints='all'))\n",
    "fig.update_layout(template = 'simple_white')\n",
    "fig.update_yaxes(title='VAF')\n",
    "fig.write_image(path + 'synonymous_size.svg', width=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evolution of neutral clones predicts that at age $t$:\n",
    "$$\\max v(t) \\approx \\frac{1/2 + \\sqrt{2\\lambda t}}{N}$$\n",
    "\n",
    "Equivalently, given a maximum observed vaf, we can estimate:\n",
    "$$ N \\approx \\frac{1/2 + \\sqrt{2\\lambda t}}{\\max v(t)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = 1/2 + np.sqrt(2*1.3*np.mean(age_list))\n",
    "denominator = np.mean(vaf_list)\n",
    "\n",
    "N = numerator / denominator\n",
    "print(f'The size of synonymous mutations estimates: N ~ {int(N)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum mean VAF per participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max_trajectory attribute for each participant\n",
    "for part in lbc:\n",
    "    part.max_trajectory = 0\n",
    "    for traj in part.trajectories:\n",
    "        mean_vaf = np.mean(traj.data.AF)\n",
    "        # Exclude cases where mean(VAF)>0.5 as these are due to LOH\n",
    "        if mean_vaf < 0.5:\n",
    "            part.max_trajectory = max(part.max_trajectory, mean_vaf)\n",
    "            \n",
    "\n",
    "cohort_max_vaf = [part.max_trajectory for part in lbc if part.max_trajectory > 0]\n",
    "\n",
    "fig = go.Figure(\n",
    "        go.Box(y=cohort_max_vaf, boxmean=True, boxpoints='all'))\n",
    "fig.update_layout(template='simple_white',\n",
    "                  title='Maximum mean VAF full cohort')\n",
    "fig.update_yaxes(title='VAF')\n",
    "fig.show()\n",
    "fig.write_image(path+'cohort_max_vaf.svg',width=500)\n",
    "print(f'Median of maximum mean VAF per participant: {round(np.median(cohort_max_vaf),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evolution of neutral clones predicts that \n",
    "\n",
    "$$ N \\approx  \\frac{\\lambda(1-2v)}{slope}$$\n",
    "\n",
    "Using maximum mean vaf as an approximation to v, and the slope from the NGF fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = round(cohort.neutral_dist.var_regr.coef_[0],4)\n",
    "l = 1.3\n",
    "median_vaf = round(np.median(cohort_max_vaf), 5)\n",
    "print(f'slope= {a} ---- median_vaf= {median_vaf}')\n",
    "HSC = l*(1-2*median_vaf)/a\n",
    "print(f'HSC ~ {HSC}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouped pathway analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gene_to_category dictionary\n",
    "\n",
    "category_to_gene = dict()\n",
    "category_to_gene['histone_regulation'] = ['EZH2', 'ASXL1', 'KMT2A', 'KDM6A']\n",
    "category_to_gene['splicing'] = ['SF3B1', 'U2AF1', 'SRSF2', 'U2AF2', 'ZRSR2']\n",
    "category_to_gene['dna_damage'] = ['TP53', 'CDKN2A']\n",
    "category_to_gene['mitogenic'] = ['KRAS', 'NF1', 'JAK2', 'JAK3']\n",
    "category_to_gene['cohesin'] = ['RAD21', 'STAG2']\n",
    "category_to_gene['tf_development'] = ['GATA2', 'RUNX1']\n",
    "category_to_gene['dna_methylation'] = ['TET2', 'DNMT3A']\n",
    "\n",
    "gene_to_category = dict()\n",
    "for k, v in category_to_gene.items():\n",
    "    for gene in v:\n",
    "        gene_to_category[gene] = k\n",
    "        \n",
    "# Create fitness distribution by category\n",
    "category_fitness = {element: [] for element in category_to_gene.keys()}\n",
    "for traj in model:\n",
    "    if traj.gene != 'NOTCH1' and traj.gene != 'NPM1'and traj.gene != 'LUC7L2' and traj.gene != 'DDX41' and traj.gene != 'BCORL1' and traj.gene != 'CUX1' and traj.gene != 'PPM1D':\n",
    "        category_fitness[gene_to_category[traj.gene]].append(traj.fitness)  \n",
    "\n",
    "# sort dictionary by mean order\n",
    "category_fitness = dict(sorted(category_fitness.items(),\n",
    "                        key=lambda item: np.mean(item[1]),\n",
    "                        reverse=True))\n",
    "\n",
    "# Plot distribution of fitness by category\n",
    "fig = go.Figure()\n",
    "for i, key in enumerate(category_fitness):\n",
    "    fig.add_trace(\n",
    "            go.Box(y=category_fitness[key],\n",
    "                   name=key, boxpoints='all', showlegend=False))\n",
    "fig.update_xaxes(linewidth=2,tickangle=-45)\n",
    "fig.update_layout(template='simple_white')\n",
    "fig.update_yaxes(linewidth=2,\n",
    "                 type='log', tickvals=[0.05,0.1,0.2,0.4])\n",
    "fig.show()\n",
    "fig.write_image(path + 'gene_category.svg')\n",
    "\n",
    "# Compute Kruskal Wallis test\n",
    "fig, dict_2 = plot.gene_statistic(category_fitness, statistic='kruskal-wallis')\n",
    "fig.show()\n",
    "fig.write_image(path + 'gene_category_kruskal.svg', width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Range of allowed N_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_surface(part_id, rtol=0.002):\n",
    "\n",
    "    # Extract fitting data from participant\n",
    "    data = neutral_fit[part_id][0]\n",
    "\n",
    "    # Select subset of data to plot using relative tolerance\n",
    "    new_data = data[np.isclose(data['error'], np.min(data.error), rtol = rtol)]\n",
    "\n",
    "    # extract data presenting the minimum fitting error\n",
    "    optimal = new_data[new_data['error']==min(new_data['error'])]\n",
    "\n",
    "    # extract dictionary optimal fitness for each variant \n",
    "    optimal_dict = dict()\n",
    "    for i, row in optimal.iterrows():\n",
    "        optimal_dict[row['Gene name']] = row['Fitness']\n",
    "\n",
    "    opt_dist = []\n",
    "    for i, row in new_data.iterrows():\n",
    "        opt_dist.append(np.abs(row['Fitness'] - optimal_dict[row['Gene name']]))\n",
    "    new_data['fitness_distance'] = opt_dist\n",
    "\n",
    "    fig = px.scatter(new_data, x= 'Origin', y='Cells',\n",
    "                     hover_data=['Fitness', 'error'],\n",
    "                     symbol='Gene name',\n",
    "                     color='fitness_distance',\n",
    "                     color_continuous_scale='Cividis'\n",
    "    )\n",
    "    fig.update_xaxes(title='Ages (years)',\n",
    "                     linewidth=2)\n",
    "    fig.update_yaxes(title='HSPCs (counts)',\n",
    "                     linewidth=2)\n",
    "    fig.update_layout(template='simple_white')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_id = 'LBC0001A'\n",
    "fig = parameter_surface(part_id, rtol=0.001)\n",
    "fig.write_image(path + f'hypersurface of paramter solutions {part_id}.svg')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_id = 'LBC0242M'\n",
    "fig = parameter_surface(part_id)\n",
    "fig.write_image(path + f'hypersurface of paramter solutions {part_id}.svg')\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARCH_env",
   "language": "python",
   "name": "arch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
